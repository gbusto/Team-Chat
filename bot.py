import os
import json
import asyncio
import websockets
import argparse
import logging
import google.generativeai as genai
from google.generativeai.types import content_types, generation_types

from openai import OpenAI

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', filename='bot.log', filemode='a')
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logging.getLogger().addHandler(console_handler)

genai.configure(api_key=os.environ['GOOGLE_DEV_API_KEY'])

MOD_TEMP = 0.2
MOD_TOP_P = 1.0

MODEL = "gemini-1.5-flash-latest"
DELAY = 5
PROCESSING_INTERVAL = 45  # Time in seconds between processing messages
MAX_RETRIES = 5  # Maximum number of retries for WebSocket connection
KEEPALIVE_INTERVAL = 120  # Increase keepalive interval

class ConversationHistory:
    def __init__(self):
        self.history = []

    def add_message(self, role, name, text):
        self.history.append({
            "role": role,
            "name": name,
            "text": text,
            "parts": [{"text": f"[{name}] {text}"}]
        })

    def get_history(self):
        return [{"role": entry["role"], "parts": entry["parts"]} for entry in self.history]

    def get_recent_history(self, limit=10):
        return self.history[-limit:]
    
class GeminiTeammate:
    def __init__(self, name, model_name, system_instructions, conversation_history, extra_params):
        temperature = float(extra_params.get("temperature"))
        top_p = float(extra_params.get("top_p"))

        logging.info(f"[+] Bot got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Bot is being initialized with temperature {temperature} and top_p {top_p}")
        config = generation_types.GenerationConfig(temperature=temperature, top_p=top_p)

        self.name = name
        self.model = genai.GenerativeModel(model_name, generation_config=config, system_instruction=system_instructions)

        # Pass in a reference to the Bot instance's "official" conversation history so we can update
        # it in self.send_message()
        self.conversation_history = conversation_history

    async def send_message(self, message, history):
        await asyncio.sleep(DELAY)
        logging.info(f"{self.name} is processing the most current chat history to respond")

        # Initialize a new chat with the current chat history every time we need a response.
        # The reason is because the FULL conversation history is managed by the Bot class,
        # of which this GeminiTeammate object is a part.
        chat = self.model.start_chat(history=history)

        # We're sending this Gemini teammate the "summary" generated by its moderator; we
        # need to send it a message in order to get a response, so the moderator takes the
        # recent chat history and creates a summary for this teammate to succinctly and 
        # accurately capture the context in which this teammate should respond.
        response = chat.send_message(message)
        reply = response.text

        # Update the "official" chat history managed by the Bot class
        self.conversation_history.add_message("model", self.name, reply)

        logging.info(f"{self.name} response: {reply}")

        return reply
    
    def llm_type(self):
        return "Gemini"
    
class GeminiModerator:
    def __init__(self, model_name, system_instructions, teammate_name, extra_params):
        temperature = float(extra_params.get("temperature"))
        top_p = float(extra_params.get("top_p"))
        
        logging.info(f"[+] Mod got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Mod is being initialized with temperature {temperature} and top_p {top_p}")
        config = generation_types.GenerationConfig(temperature=temperature, top_p=top_p)
        self.model = genai.GenerativeModel(model_name, generation_config=config, system_instruction=system_instructions)
        self.teammate_name = teammate_name
        self.chat = self.model.start_chat()

    async def should_speak_next(self, chat_history):
        await asyncio.sleep(DELAY)
        recent_history = chat_history[-10:]  # Get the last 10 messages
        history_text = "\n".join([f"{entry['parts'][0]['text']}" for entry in recent_history])

        # Give the moderator a solid prompt message; it uses more tokens but should hopefully result in
        # improved conversation accuracy overall
        prompt = f"Based on the following conversation, should {self.teammate_name} speak next?\n\n{history_text}\n\If yes, please summarize what is being asked of {self.teammate_name} respond with 'YES|Your name is {self.teammate_name} and you should respond in this chat. Here is what is being asked of you: <summary>'. If {self.teammate_name} should not speak, respond with 'NO'"

        # We basically want to the moderator to look at the recent chat history (since there have likely been
        # many messages sent since the last time it responded) and determine if the teammate for whom
        # they're moderating should speak next. If yes, they should give that teammate a summary of what's been
        # going on. This summary generation thing is really only necessary for Gemini's API the way it currently
        # works. You need to init the chat `model.start_chat(history=[])` with history, then give it a message
        # to respond to: `reponse = chat.send_message("something")`. It doesn't work like this with OpenAI; for that,
        # all we need to do is pass in the conversation history and let it run and do its thing.

        response = self.chat.send_message(prompt)

        response = response.text.strip()

        logging.info(f"Moderator for {self.teammate_name}'s response: {response}")

        if '|' in response:
            answer, summary = response.split('|')
            if answer.lower() in ["yes", "y"]:
                return True, summary
            return False, ""
        
        else:
            if response.startswith("NO") or response.startswith("no") or response.startswith("No"):
                return False, ""

        logging.warn(f"Moderator for {self.teammate_name} had an invalid response: {response}")
        return False, ""
        
    def llm_type(self):
        return "gemini"
    
# class OpenAITeammate:
#     def __init__(self, name, model_name, system_instructions, conversation_history, extra_params):
#         """
#         model_name, system_instructions, conversation_history will all likely be None/empty for OpenAI teammates since we're loading assistants already created in OpenAI interface
#         """
#         asst_id = extra_params.get("asst_id")

#         self.name = name

#         self.client = OpenAI()
#         self.assistant = self.client.beta.assistants.retrieve(asst_id)

#         system_instructions = self.assistant.instructions
#         temperature = self.assistant.temperature
#         top_p = self.assistant.top_p

#         self.conversation_history = conversation_history

#         logging.info(f"[+] Bot got system instruction: {system_instructions[:20]}")
#         logging.info(f"[+] Bot is being initialized with temperature {temperature} and top_p {top_p}")
        
#     async def send_message(self, message):
#         await asyncio.sleep(DELAY)
#         logging.info(f"{self.name} is processing the message: {message}")



#         self.conversation_history.add_message("user", self.name, message)
#         response = self.chat.send_message(message)
#         reply = response.text
#         self.conversation_history.add_message("model", self.name, reply)
#         logging.info(f"{self.name} response: {reply}")
#         return reply
    
#     def llm_type(self):
#         return "gpt"


class AI_Teammate:
    def __init__(self, name, model_name, system_instructions, conversation_history, extra_params, llm=GeminiTeammate):
        self.llm = llm(
            name=name,
            model_name=model_name,
            system_instructions=system_instructions,
            conversation_history=conversation_history,
            extra_params=extra_params
        )

        logging.info(f"[+] Created an a teammate with LLM of type {self.llm.llm_type()}")

    async def send_message(self, message, history):
        return await self.llm.send_message(message=message, history=history)

class AIModerator:
    def __init__(self, model_name, system_instructions, teammate_name, extra_params, llm=GeminiModerator):
        self.llm = llm(
            model_name=model_name,
            system_instructions=system_instructions,
            teammate_name=teammate_name,
            extra_params=extra_params
        )

        logging.info(f"[+] Created a moderator for teammate {teammate_name} with LLM of type {self.llm.llm_type()}")

    async def should_speak_next(self, chat_history):
        return await self.llm.should_speak_next(chat_history)

class Bot:
    def __init__(self, _id, name, host, port, hub_uri, bot_instructions, mod_instructions, teammate_extra_params, mod_extra_params):
        self._id = _id
        self.name = name
        self.host = host
        self.port = port
        self.hub_uri = hub_uri

        self.conversation_history = ConversationHistory()
        self.teammate = AI_Teammate(name=name, model_name=MODEL, system_instructions=bot_instructions,
                                    conversation_history=self.conversation_history, extra_params=teammate_extra_params)
        self.moderator = AIModerator(model_name=MODEL, system_instructions=mod_instructions, teammate_name=name, extra_params=mod_extra_params)
        self.message_queue = asyncio.Queue()

    async def connect(self):
        retries = 0
        while retries < MAX_RETRIES:
            try:
                async with websockets.connect(self.hub_uri, ping_interval=KEEPALIVE_INTERVAL) as websocket:
                    # Register bot with the hub
                    connect_msg = {
                        "type": "ai_connect",
                        "name": self.name,
                        "id": self._id,
                        "origin": "ai",
                        "host": self.host,
                        "port": self.port
                    }
                    await websocket.send(json.dumps(connect_msg))

                    # Start the message processing task
                    asyncio.create_task(self.process_messages(websocket))

                    async for message in websocket:
                        await self.handle_message(message)
            except websockets.exceptions.ConnectionClosedError as e:
                logging.error(f"{self.name} WebSocket connection error: {e}")
                retries += 1
                logging.info(f"{self.name} Retrying connection ({retries}/{MAX_RETRIES})...")
                await asyncio.sleep(5)
            except Exception as e:
                logging.error(f"Unexpected error: {e}")
                break

    async def handle_message(self, message_obj_str):
        # Some kind of message has been received via websocket
        try:
            message = json.loads(message_obj_str)
            # Get the message type so we know what to do with it
            msg_type = message.get("type")

            if msg_type == "msg_recvd":
                # If it's a message saying that a new message has been sent to the chat...
                sender = message.get("from")
                msg = message.get("message")

                # Add it to the chat history (even if it's an event)...
                self.conversation_history.add_message("user", sender, msg)

                # And as long as it's not an event notification, add it to the message queue for the next time this bot is able to process messages
                if not msg.startswith("[EVENT]"):
                    await self.message_queue.put(message)
        except Exception as e:
            logging.error(f"Error handling message: {e}")

    async def process_messages(self, websocket):
        # This runs every PROCESSING_INTERVAL seconds

        while True:
            await asyncio.sleep(PROCESSING_INTERVAL)

            # Get the current chat history at this moment
            # TODO: Change conversation_history.get_history() to be Gemini-specific;
            # ... then I can add one for OpenAI/GPT, and we can have a specific
            # history format that matches each of the APIs. Or, have each API modify
            # history to its ideal state
            current_chat_history = self.conversation_history.get_history()

            # Take the last 10 messages for the moderator
            recent_history = self.conversation_history.get_recent_history()

            try:
                # Ask the teammate moderator if this teammate should speak next...
                # It will return a tuple (bool, str)
                # The bool determines whether or not the teammate should respond
                # If yes, the moderator should have generated a summary to give the teammate to help them respond with accuracy
                should_speak, summary = await self.moderator.should_speak_next(recent_history)
                if should_speak:
                    # If the bot should speak next, send it the entire chat history
                    # The 'summary' is the message we send it. This is how it works for Gemini, but when we
                    # create the ChatGPT teammate, I don't think this will be necessary; it will just need the
                    # chat history
                    response = await self.teammate.send_message(message=summary, history=current_chat_history)
                    response_msg = {
                        "type": "msg_recvd",
                        "from": self._id,
                        "origin": "ai",
                        "message": response
                    }
                    await websocket.send(json.dumps(response_msg))
            except websockets.exceptions.ConnectionClosedError as e:
                logging.error(f"{self.name} WebSocket connection error: {e}")
                await websocket.close()
                break
            except Exception as e:
                logging.error(f"Error processing messages: {e}")

def main():
    parser = argparse.ArgumentParser(description="AI Bot")
    parser.add_argument("--id", type=str, required=True, help="Unique identifier for the bot")
    parser.add_argument("--name", type=str, required=True, help="Name of the bot")
    parser.add_argument("--host", type=str, required=True, help="Host IP address")
    parser.add_argument("--port", type=int, required=True, help="Port number")
    parser.add_argument("--hub_uri", type=str, required=True, help="WebSocket URI of the hub")
    parser.add_argument("--instruction-file", type=str, required=True, help="The txt file containing instructions for this bot")
    parser.add_argument("--moderator-instruction-file", type=str, required=True, help="The moderator instruction file")
    parser.add_argument("--mod-extra-params", type=str, required=True, help="A stringified JSON object containing extra params for a moderator")
    parser.add_argument("--teammate-extra-params", type=str, required=True, help="A stringified JSON object containing extra params for a teammate")

    args = parser.parse_args()

    inst_file = args.instruction_file
    m_inst_file = args.moderator_instruction_file

    if not os.path.exists(inst_file):
        logging.error(f"[!] Instruction file {inst_file} does not exist!")
        exit(-1)
    if not os.path.exists(m_inst_file):
        logging.error(f"[!] Moderator instruction file {m_inst_file} does not exist!")

    with open(inst_file, "r") as f:
        inst = f.read()

    with open(m_inst_file, "r") as f:
        m_inst = f.read()

    tep = json.loads(args.teammate_extra_params)
    mep = json.loads(args.mod_extra_params)

    bot = Bot(_id=args.id, name=args.name, host=args.host, port=args.port, hub_uri=args.hub_uri,
              bot_instructions=inst, mod_instructions=m_inst,
              teammate_extra_params=tep, mod_extra_params=mep)

    try:
        asyncio.run(bot.connect())
    except KeyboardInterrupt:
        logging.info("Bot process interrupted and stopped.")

if __name__ == "__main__":
    main()
