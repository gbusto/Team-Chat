import os
import json
import asyncio
import websockets
import argparse
import logging
import google.generativeai as genai
from google.generativeai.types import content_types, generation_types

from openai import OpenAI

# Configure logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s', filename='bot.log', filemode='a')
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logging.getLogger().addHandler(console_handler)

genai.configure(api_key=os.environ['GOOGLE_DEV_API_KEY'])

MOD_TEMP = 0.2
MOD_TOP_P = 1.0

MODEL = "gemini-1.5-flash-latest"
DELAY = 45
DELAY_OPENAI = 20
PROCESSING_INTERVAL = 15  # Time in seconds between processing messages
MAX_RETRIES = 5  # Maximum number of retries for WebSocket connection
KEEPALIVE_INTERVAL = 120  # Increase keepalive interval

RECENT_HISTORY_NUMBER = 25

class ConversationHistory:
    def __init__(self):
        self.history = []

    def add_message(self, role, name, text):
        self.history.append({
            "role": role,
            "name": name,
            "text": text,
            "parts": [{"text": f"[{name}] {text}"}]
        })

    def get_history_gemini(self, limit=-1):
        if limit < 0:
            history = self.history
        else:
            history = self.history[:limit]

        modified_history = []
        for entry in history:
            role = entry["role"]
            parts = entry["parts"]

            if role == "ai":
                role = "model"

            modified_history.append({
                "role": role,
                "parts": parts
            })
        
        return modified_history
    
    def get_history_openai(self, limit=-1):
        if limit < 0:
            history = self.history
        else:
            history = self.history[:limit]

        modified_history = []
        for entry in history:
            role = entry["role"]
            content = entry["text"]

            if role == "ai":
                role = "assistant"

            modified_history.append({
                "role": role,
                "content": content
            })
        
        return modified_history
    
class GeminiTeammate:
    def __init__(self, name, model_name, system_instructions, conversation_history, extra_params):
        temperature = float(extra_params.get("temperature"))
        top_p = float(extra_params.get("top_p"))

        logging.info(f"[+] Bot got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Bot is being initialized with temperature {temperature} and top_p {top_p}")
        config = generation_types.GenerationConfig(temperature=temperature, top_p=top_p)

        self.name = name
        self.model = genai.GenerativeModel(model_name, generation_config=config, system_instruction=system_instructions)

        # Pass in a reference to the Bot instance's "official" conversation history so we can update
        # it in self.send_message()
        self.conversation_history = conversation_history

    async def allow_send_message(self, moderator):
        # `moderator` should ALWAYS be a GeminiModerator type
        
        # Get the current chat history at this moment
        current_chat_history = self.conversation_history.get_history_gemini()

        # Take the last 10 messages for the moderator
        recent_history = current_chat_history[:10]

        # Ask the teammate moderator if this teammate should speak next...
        # It will return a tuple (bool, str)
        # The bool determines whether or not the teammate should respond
        # If yes, the moderator should have generated a summary to give the teammate to help them respond with accuracy
        should_speak, summary = await moderator.should_speak_next(recent_history)
        if should_speak:
            # If the bot should speak next, send it the entire chat history
            # The 'summary' is the message we send it. This is how it works for Gemini, but when we
            # create the ChatGPT teammate, I don't think this will be necessary; it will just need the
            # chat history
            response = await self.send_message(message=summary, history=current_chat_history)

            return response
        
        return None

    async def send_message(self, message, history):
        await asyncio.sleep(DELAY)
        logging.info(f"{self.name} is processing the most current chat history to respond")

        # Initialize a new chat with the current chat history every time we need a response.
        # The reason is because the FULL conversation history is managed by the Bot class,
        # of which this GeminiTeammate object is a part.
        chat = self.model.start_chat(history=history)

        # We're sending this Gemini teammate the "summary" generated by its moderator; we
        # need to send it a message in order to get a response, so the moderator takes the
        # recent chat history and creates a summary for this teammate to succinctly and 
        # accurately capture the context in which this teammate should respond.
        response = chat.send_message(message)
        reply = response.text

        # If they have a response to send in the chat, add it to the conversation history
        self.conversation_history.add_message("ai", self.name, reply)

        logging.info(f"[{self.llm_type()}] {self.name} response: {reply}")

        return reply
    
    def llm_type(self):
        return "Gemini"
    
class GeminiModerator:
    def __init__(self, model_name, system_instructions, teammate_name, extra_params):
        temperature = float(extra_params.get("temperature"))
        top_p = float(extra_params.get("top_p"))
        
        logging.info(f"[+] Mod got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Mod is being initialized with temperature {temperature} and top_p {top_p}")
        config = generation_types.GenerationConfig(temperature=temperature, top_p=top_p)
        self.model = genai.GenerativeModel(model_name, generation_config=config, system_instruction=system_instructions)
        self.teammate_name = teammate_name
        self.chat = self.model.start_chat()

    async def should_speak_next(self, chat_history):
        await asyncio.sleep(DELAY)
        recent_history = chat_history[-RECENT_HISTORY_NUMBER:]  # Get the last 10 messages
        history_text = "\n".join([f"{entry['parts'][0]['text']}" for entry in recent_history])

        # Give the moderator a solid prompt message; it uses more tokens but should hopefully result in
        # improved conversation accuracy overall
        prompt = f"Based on the following conversation, should {self.teammate_name} speak next?\n\n{history_text}\n\If yes, please summarize what is being asked of {self.teammate_name} respond with 'YES|Your name is {self.teammate_name} and you should respond in this chat. Here is what is being asked of you: <summary>'. If {self.teammate_name} should not speak, respond with 'NO'"

        # We basically want to the moderator to look at the recent chat history (since there have likely been
        # many messages sent since the last time it responded) and determine if the teammate for whom
        # they're moderating should speak next. If yes, they should give that teammate a summary of what's been
        # going on. This summary generation thing is really only necessary for Gemini's API the way it currently
        # works. You need to init the chat `model.start_chat(history=[])` with history, then give it a message
        # to respond to: `reponse = chat.send_message("something")`. It doesn't work like this with OpenAI; for that,
        # all we need to do is pass in the conversation history and let it run and do its thing.

        response = self.chat.send_message(prompt)

        response = response.text.strip()

        logging.info(f"Moderator for {self.teammate_name}'s response: {response}")

        if '|' in response:
            answer, summary = response.split('|')
            if answer.lower() in ["yes", "y"]:
                return True, summary
            return False, ""
        
        else:
            if response.startswith("NO") or response.startswith("no") or response.startswith("No"):
                return False, ""

        logging.warn(f"Moderator for {self.teammate_name} had an invalid response: {response}")
        return False, ""
        
    def llm_type(self):
        return "gemini"
    
class OpenAIAPIHandler:
    def __init__(self, assistant_id):
        self.client = OpenAI()
        self.assistant = self.client.beta.assistants.retrieve(assistant_id)

    async def interact(self, history):
        # TODO: Eventually add truncation strategy
        run = self.client.beta.threads.create_and_run(
            assistant_id=self.assistant.id,
            thread={
                "messages": history
            }
        )

        run_id = run.id
        thread_id = run.thread_id

        while run.status not in ["cancelled", "failed", "completed", "expired"]:
            logging.info(f"OpenAI API Run is not done yet ({run.status})...")
            await asyncio.sleep(1)
            run = self.client.beta.threads.runs.retrieve(
                thread_id=thread_id,
                run_id=run_id
            )

        if run.status == "completed":
            messages = self.client.beta.threads.messages.list(
                thread_id=thread_id,
                run_id=run_id
            )

            if messages.data:
                constructed_msg = ""
                for msg in messages.data:
                    text = msg.content[0].text.value             
                    constructed_msg += text

                logging.info(f"[gpt] Assistant Run - response:\n{constructed_msg}")

                return constructed_msg
            
            else:
                logging.warn(f"[gpt] Assistant Run doesn't appear to have generated messages")

        else:
            logging.warn(f"[gpt] Assistant Run ended with status {run.status}")
            
        return None
    
class OpenAITeammate:
    def __init__(self, name, model_name, system_instructions, conversation_history, extra_params):
        """
        model_name, system_instructions, conversation_history will all likely be None/empty for OpenAI teammates since we're loading assistants already created in OpenAI interface
        """
        asst_id = extra_params.get("assistant_id")

        self.name = name

        self.handler = OpenAIAPIHandler(assistant_id=asst_id)
        assistant = self.handler.assistant

        model_name = assistant.model
        system_instructions = assistant.instructions
        temperature = assistant.temperature
        top_p = assistant.top_p

        self.conversation_history = conversation_history

        logging.info(f"[+] OpenAI [{model_name}] got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Bot is being initialized with temperature {temperature} and top_p {top_p}")

    async def allow_send_message(self, moderator):
        # `moderator` should ALWAYS be a OpenAIModerator type

        # Get the current chat history at this moment
        current_chat_history = self.conversation_history.get_history_openai()

        # Take the last 10 messages for the moderator
        recent_history = current_chat_history[:10]

        should_speak = await moderator.should_speak_next(recent_history)
        if should_speak:
            response = await self.send_message(history=current_chat_history)

            return response
        
        return None
        
    async def send_message(self, history):
        await asyncio.sleep(DELAY)
        logging.info(f"[{self.llm_type()}] {self.name} is processing chat history")

        response = await self.handler.interact(history)
        self.conversation_history.add_message("ai", self.name, response)

        return response
    
    def llm_type(self):
        return "gpt"
    
class OpenAIModerator:
    def __init__(self, model_name, system_instructions, teammate_name, extra_params):
        """
        model_name, system_instructions, conversation_history will all likely be None/empty for OpenAI teammates since we're loading assistants already created in OpenAI interface
        """
        asst_id = extra_params.get("assistant_id")

        self.handler = OpenAIAPIHandler(assistant_id=asst_id)
        assistant = self.handler.assistant

        self.teammate_name = teammate_name

        model_name = assistant.model
        system_instructions = assistant.instructions
        temperature = assistant.temperature
        top_p = assistant.top_p

        logging.info(f"[+] OpenAI [{model_name}] got system instruction: {system_instructions[:20]}")
        logging.info(f"[+] Mod is being initialized with temperature {temperature} and top_p {top_p}")

    async def should_speak_next(self, chat_history):
        await asyncio.sleep(DELAY_OPENAI)

        recent_history = chat_history[-RECENT_HISTORY_NUMBER:] # Get the last 10 messages
        print("MOD")
        print(recent_history)
        prompt = f"Based on the conversation history you have access to, should {self.teammate_name} speak next? Respond with a single word: either YES or NO."

        recent_history.append({
            "role": "user",
            "content": prompt,
        })

        response = await self.handler.interact(recent_history)

        if response.lower() in ["yes", "y"]:
            return True
        
        elif response.lower() in ["no", "n"]:
            return False
        
        logging.warn(f"[{self.llm_type()}] Moderator for {self.teammate_name} doesn't seemd to have generated a proper response: {response}")

        return False
    
    def llm_type(self):
        return "gpt"

class AI_Teammate:
    def __init__(self, name, model_name, system_instructions, conversation_history, extra_params, llm=GeminiTeammate):
        self.llm = llm(
            name=name,
            model_name=model_name,
            system_instructions=system_instructions,
            conversation_history=conversation_history,
            extra_params=extra_params
        )

        logging.info(f"[+] Created an a teammate with LLM of type {self.llm.llm_type()}")

    async def allow_send_message(self, moderator):
        return await self.llm.allow_send_message(moderator=moderator)

    async def send_message(self, message, history):
        return await self.llm.send_message(message=message, history=history)

class AIModerator:
    def __init__(self, model_name, system_instructions, teammate_name, extra_params, llm=GeminiModerator):
        self.llm = llm(
            model_name=model_name,
            system_instructions=system_instructions,
            teammate_name=teammate_name,
            extra_params=extra_params
        )

        logging.info(f"[+] Created a moderator for teammate {teammate_name} with LLM of type {self.llm.llm_type()}")

    async def should_speak_next(self, chat_history):
        return await self.llm.should_speak_next(chat_history)

class Bot:
    def __init__(self, _id, name, host, port, hub_uri, bot_instructions, mod_instructions, teammate_extra_params, mod_extra_params, llm):
        self._id = _id
        self.name = name
        self.host = host
        self.port = port
        self.hub_uri = hub_uri

        if llm == 'gemini':
            llm_type_teammate = GeminiTeammate
            llm_type_moderator = GeminiModerator
        elif llm == 'gpt':
            llm_type_teammate = OpenAITeammate
            llm_type_moderator = OpenAIModerator
        else:
            raise Exception("No LLM provided!")

        self.conversation_history = ConversationHistory()
        self.teammate = AI_Teammate(name=name, model_name=MODEL, system_instructions=bot_instructions,
                                    conversation_history=self.conversation_history, extra_params=teammate_extra_params,
                                    llm=llm_type_teammate)
        self.moderator = AIModerator(model_name=MODEL, system_instructions=mod_instructions, teammate_name=name, extra_params=mod_extra_params, llm=llm_type_moderator)
        self.message_queue = asyncio.Queue()

    async def connect(self):
        retries = 0
        while retries < MAX_RETRIES:
            try:
                async with websockets.connect(self.hub_uri, ping_interval=KEEPALIVE_INTERVAL) as websocket:
                    # Register bot with the hub
                    connect_msg = {
                        "type": "ai_connect",
                        "name": self.name,
                        "id": self._id,
                        "origin": "ai",
                        "host": self.host,
                        "port": self.port
                    }
                    await websocket.send(json.dumps(connect_msg))

                    # Start the message processing task
                    asyncio.create_task(self.process_messages(websocket))

                    async for message in websocket:
                        await self.handle_message(message)
            except websockets.exceptions.ConnectionClosedError as e:
                logging.error(f"{self.name} WebSocket connection error: {e}")
                retries += 1
                logging.info(f"{self.name} Retrying connection ({retries}/{MAX_RETRIES})...")
                await asyncio.sleep(5)
            except Exception as e:
                logging.error(f"Unexpected error: {e}")
                break

    async def handle_message(self, message_obj_str):
        # Some kind of message has been received via websocket
        try:
            message = json.loads(message_obj_str)
            # Get the message type so we know what to do with it
            msg_type = message.get("type")

            if msg_type == "msg_recvd":
                # If it's a message saying that a new message has been sent to the chat...
                sender = message.get("from")
                msg = message.get("message")

                # Add it to the chat history (even if it's an event)...
                self.conversation_history.add_message("user", sender, msg)

                # And as long as it's not an event notification, add it to the message queue for the next time this bot is able to process messages
                if not msg.startswith("[EVENT]"):
                    await self.message_queue.put(message)
        except Exception as e:
            logging.error(f"Error handling message: {e}")

    async def process_messages(self, websocket):
        # This runs every PROCESSING_INTERVAL seconds

        while True:
            await asyncio.sleep(PROCESSING_INTERVAL)

            try:
                # Give the teammate a chance to see if it should respond
                response = await self.teammate.allow_send_message(self.moderator)

                if response:
                    response_msg = {
                        "type": "msg_recvd",
                        "from": self._id,
                        "origin": "ai",
                        "message": response
                    }
                    await websocket.send(json.dumps(response_msg))
            except websockets.exceptions.ConnectionClosedError as e:
                logging.error(f"{self.name} WebSocket connection error: {e}")
                await websocket.close()
                break
            except Exception as e:
                logging.error(f"Error processing messages: {e}")

def main():
    parser = argparse.ArgumentParser(description="AI Bot")
    parser.add_argument("--id", type=str, required=True, help="Unique identifier for the bot")
    parser.add_argument("--name", type=str, required=True, help="Name of the bot")
    parser.add_argument("--host", type=str, required=True, help="Host IP address")
    parser.add_argument("--port", type=int, required=True, help="Port number")
    parser.add_argument("--hub_uri", type=str, required=True, help="WebSocket URI of the hub")
    parser.add_argument("--instruction-file", type=str, required=True, help="The txt file containing instructions for this bot")
    parser.add_argument("--moderator-instruction-file", type=str, required=True, help="The moderator instruction file")
    parser.add_argument("--mod-extra-params", type=str, required=True, help="A stringified JSON object containing extra params for a moderator")
    parser.add_argument("--teammate-extra-params", type=str, required=True, help="A stringified JSON object containing extra params for a teammate")
    parser.add_argument("--llm", type=str, required=True, help="The LLM type. Right now, options are 'gemini' or 'gpt'")

    args = parser.parse_args()

    inst_file = args.instruction_file
    m_inst_file = args.moderator_instruction_file

    if not os.path.exists(inst_file):
        logging.error(f"[!] Instruction file {inst_file} does not exist!")
        exit(-1)
    if not os.path.exists(m_inst_file):
        logging.error(f"[!] Moderator instruction file {m_inst_file} does not exist!")

    with open(inst_file, "r") as f:
        inst = f.read()

    with open(m_inst_file, "r") as f:
        m_inst = f.read()

    tep = json.loads(args.teammate_extra_params)
    mep = json.loads(args.mod_extra_params)

    bot = Bot(_id=args.id, name=args.name, host=args.host, port=args.port, hub_uri=args.hub_uri,
              bot_instructions=inst, mod_instructions=m_inst,
              teammate_extra_params=tep, mod_extra_params=mep,
              llm=args.llm)

    try:
        asyncio.run(bot.connect())
    except KeyboardInterrupt:
        logging.info("Bot process interrupted and stopped.")

if __name__ == "__main__":
    main()
